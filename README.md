# ğŸ“Š Tech Challenge - Pipeline de Engenharia de Dados (B3)

Este projeto compÃµe a entrega da **Fase 2** da PÃ³s-Tech em Data Analytics. O objetivo foi construir um pipeline de dados completo na nuvem AWS para a ingestÃ£o, processamento e anÃ¡lise de dados da Bolsa de Valores (B3).

## ğŸš€ Arquitetura da SoluÃ§Ã£o

O pipeline foi desenhado para ser **Event-Driven** (orientado a eventos), garantindo automaÃ§Ã£o total desde a ingestÃ£o atÃ© a disponibilizaÃ§Ã£o dos dados.

**Fluxo de Dados:**
1.  **IngestÃ£o (Local):** Script Python extrai dados de aÃ§Ãµes (PETR4, VALE3, ITUB4) via `yfinance`.
2.  **Armazenamento Raw (S3):** Os dados brutos sÃ£o enviados para a camada `raw/` no Amazon S3 em formato Parquet.
3.  **AutomaÃ§Ã£o (Lambda):** O upload no S3 aciona automaticamente uma funÃ§Ã£o **AWS Lambda**.
4.  **Processamento (Glue):** A Lambda inicia um Job do **AWS Glue (Spark)**.
5.  **TransformaÃ§Ã£o:** O Glue lÃª os dados, limpa, calcula a mÃ©dia mÃ³vel de 7 dias e renomeia colunas.
6.  **Armazenamento Refined (S3):** Os dados processados sÃ£o salvos na camada `refined/` particionados por Data e Ticker.
7.  **Analytics (Athena):** Os dados sÃ£o catalogados e ficam disponÃ­veis para consulta SQL no **AWS Athena**.

---

## ğŸ› ï¸ Tecnologias Utilizadas

* **Linguagem:** Python 3.9+
* **Bibliotecas:** `boto3`, `pandas`, `yfinance`, `pyarrow`
* **AWS Services:**
    * **S3:** Data Lake (Camadas Raw e Refined).
    * **Lambda:** Gatilho de eventos (Trigger).
    * **Glue:** ETL Serverless com PySpark.
    * **Athena:** Consultas Ad-hoc (SQL).
    * **IAM:** GestÃ£o de permissÃµes e seguranÃ§a.

---

## ğŸ“‚ Estrutura do Projeto

```text
/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingestao_b3.py        # Script local para extraÃ§Ã£o e envio ao S3
â”‚   â”œâ”€â”€ lambda_function.py    # CÃ³digo da funÃ§Ã£o Lambda (Trigger)
â”‚   â””â”€â”€ glue_job_script.py    # Script PySpark executado no AWS Glue
â”œâ”€â”€ requirements.txt          # DependÃªncias do projeto
â””â”€â”€ README.md                 # DocumentaÃ§Ã£o